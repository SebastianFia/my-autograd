# my-autograd

My Python implementation of the *Autograd* algorithm, which sits at the core of libraries such as *PyTorch* and *Tensorflow*. Inspired by the repo karpathy/micrograd.
This is an unoptimized and simplified version that works only for scalar values, not for tensors.
In the nn.py we implement an MLP, and in optim.py we implement an SGD optimizer. An example training on a toy dataset can be seen in example.py.

